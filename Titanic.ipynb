{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NzALKGrK7Es",
        "colab_type": "text"
      },
      "source": [
        "# Titanic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UipCTgPqLGch",
        "colab_type": "text"
      },
      "source": [
        "## Librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj4rWPkxK2CK",
        "colab_type": "code",
        "outputId": "cbbf54c1-fc43-42bb-e087-a6a8790d9941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sl\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(tf.__version__)\n",
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(sl.__version__)"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "1.0.4\n",
            "1.18.4\n",
            "0.22.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_qF84goLLsn",
        "colab_type": "text"
      },
      "source": [
        "## Download dei Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkJ8c9cMLPRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url_train = 'https://raw.githubusercontent.com/rirolli/Titanic/master/train.csv'\n",
        "url_test = 'https://raw.githubusercontent.com/rirolli/Titanic/master/test.csv'\n",
        "\n",
        "titanic_load_train = pd.read_csv(url_train)\n",
        "titanic_load_test = pd.read_csv(url_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJKoI3SiL8u8",
        "colab_type": "code",
        "outputId": "29913492-e83f-4ea8-e26c-83c1bc4ad9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Stampa delle prima 5 righe del dataset di train\n",
        "titanic_load_train.head()"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Y1iLExMJD6",
        "colab_type": "text"
      },
      "source": [
        "## Ottimizzazione del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI-ejfNWMMA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rimozione della colonna dei nomi delle persone in quanto non Ã¨ necessaria\n",
        "# ai fini dell'apprendimento della rete\n",
        "titanic_load_train.pop('Name')\n",
        "titanic_load_test.pop('Name')\n",
        "\n",
        "# Codifica di tutti i dati dei dataset titanic_load_train e titanic_load_test\n",
        "for elem_train in titanic_load_train:\n",
        "  titanic_load_train[elem_train] = (pd.Categorical(titanic_load_train[elem_train])).codes\n",
        "\n",
        "for elem_test in titanic_load_test:\n",
        "  titanic_load_test[elem_test] = (pd.Categorical(titanic_load_test[elem_test])).codes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGh_SGY0MWF6",
        "colab_type": "code",
        "outputId": "a117a03f-c11a-4f14-d51c-7e4ef3b96d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Stampa delle prima 5 righe del dataset di train\n",
        "titanic_load_train.head()"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>523</td>\n",
              "      <td>18</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>596</td>\n",
              "      <td>207</td>\n",
              "      <td>81</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>669</td>\n",
              "      <td>41</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>189</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>472</td>\n",
              "      <td>43</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  Sex  ...  Ticket  Fare  Cabin  Embarked\n",
              "0            0         0       2    1  ...     523    18     -1         2\n",
              "1            1         1       0    0  ...     596   207     81         0\n",
              "2            2         1       2    0  ...     669    41     -1         2\n",
              "3            3         1       0    0  ...      49   189     55         2\n",
              "4            4         0       2    1  ...     472    43     -1         2\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgeJkU0CPaSM",
        "colab_type": "code",
        "outputId": "b173c771-2fa2-43fa-80b8-229e2e150edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Stampa delle prima 5 righe del dataset di test\n",
        "titanic_load_test.head()"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>152</td>\n",
              "      <td>24</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>73</td>\n",
              "      <td>41</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>147</td>\n",
              "      <td>34</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>138</td>\n",
              "      <td>46</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass  Sex  Age  SibSp  Parch  Ticket  Fare  Cabin  Embarked\n",
              "0            0       2    1   44      0      0     152    24     -1         1\n",
              "1            1       2    0   60      1      0     221     5     -1         2\n",
              "2            2       1    1   74      0      0      73    41     -1         1\n",
              "3            3       2    1   34      0      0     147    34     -1         2\n",
              "4            4       2    0   27      1      1     138    46     -1         2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0i_G4xHMfkl",
        "colab_type": "text"
      },
      "source": [
        "### Ripartizione del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIuoLXQNMjiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dividiamo il train set in due parti cosÃ¬ da poter addestrare la rete:\n",
        "# y che contiene solo la colonna 'Survived' che consiste nella soluzione al problema\n",
        "# X che contiene tutte le altre colonne.\n",
        "y = titanic_load_train.Survived\n",
        "X = titanic_load_train.drop(labels=['Survived'], axis=1)\n",
        "\n",
        "# I Dati vengono, a loro volta, divisi in altre due parti: una per\n",
        "# l'addestramento e una per la valutazione della rete.\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2)\n",
        "\n",
        "# I dati vengono poi memorizzati dentro un Dataset di Tensorflow per sfruttarli\n",
        "# per l'addestramento. Questa stessa operazione viene poi fatta anche per il\n",
        "# test set e per l'evaluetion set.\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))).shuffle(len(X_train)).batch(1)\n",
        "val_dataset = (tf.data.Dataset.from_tensor_slices((X_val.values, y_val.values))).shuffle(len(X_val)).batch(1)\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(titanic_load_test.values)).batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WcDn3ZnTqpY",
        "colab_type": "text"
      },
      "source": [
        "## La rete neurale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzlzXYxtRURi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_inputs = X_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqaMTc11TvG3",
        "colab_type": "code",
        "outputId": "aad216f3-9375-4897-d5be-929752119eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "  tf.keras.layers.Dense(32, input_dim=num_inputs, activation='relu'),\n",
        "  tf.keras.layers.Dense(32, activation='relu'),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_61 (Dense)             (None, 32)                352       \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,441\n",
            "Trainable params: 1,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfURHtD1WVug",
        "colab_type": "text"
      },
      "source": [
        "## Addestramento della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mIN4eC_UR8Z",
        "colab_type": "code",
        "outputId": "760babf4-d958-46ea-ff4e-46c602c979fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Addestramento della rete neurale\n",
        "model.fit(train_dataset, epochs=300)"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 4.4466 - accuracy: 0.5913\n",
            "Epoch 2/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 1.9370 - accuracy: 0.6278\n",
            "Epoch 3/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 1.6559 - accuracy: 0.6250\n",
            "Epoch 4/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 1.5281 - accuracy: 0.6461\n",
            "Epoch 5/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 1.2962 - accuracy: 0.6489\n",
            "Epoch 6/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 1.1997 - accuracy: 0.6376\n",
            "Epoch 7/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 1.0091 - accuracy: 0.6517\n",
            "Epoch 8/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 1.0490 - accuracy: 0.6461\n",
            "Epoch 9/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.8500 - accuracy: 0.6685\n",
            "Epoch 10/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.8106 - accuracy: 0.6643\n",
            "Epoch 11/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.8249 - accuracy: 0.6812\n",
            "Epoch 12/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.7538 - accuracy: 0.6587\n",
            "Epoch 13/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.7034 - accuracy: 0.6826\n",
            "Epoch 14/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.6904 - accuracy: 0.6671\n",
            "Epoch 15/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.6650 - accuracy: 0.6910\n",
            "Epoch 16/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.6119 - accuracy: 0.6980\n",
            "Epoch 17/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.6211 - accuracy: 0.6994\n",
            "Epoch 18/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.6135 - accuracy: 0.6826\n",
            "Epoch 19/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.6206 - accuracy: 0.6966\n",
            "Epoch 20/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.6251 - accuracy: 0.7065\n",
            "Epoch 21/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5708 - accuracy: 0.7303\n",
            "Epoch 22/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.6050 - accuracy: 0.6938\n",
            "Epoch 23/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5772 - accuracy: 0.7177\n",
            "Epoch 24/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5945 - accuracy: 0.6994\n",
            "Epoch 25/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5756 - accuracy: 0.7008\n",
            "Epoch 26/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5651 - accuracy: 0.7065\n",
            "Epoch 27/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5513 - accuracy: 0.7233\n",
            "Epoch 28/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5444 - accuracy: 0.7121\n",
            "Epoch 29/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5467 - accuracy: 0.7191\n",
            "Epoch 30/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5452 - accuracy: 0.7247\n",
            "Epoch 31/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5555 - accuracy: 0.7135\n",
            "Epoch 32/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5371 - accuracy: 0.7163\n",
            "Epoch 33/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7065\n",
            "Epoch 34/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5325 - accuracy: 0.7233\n",
            "Epoch 35/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5186 - accuracy: 0.7430\n",
            "Epoch 36/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5177 - accuracy: 0.7247\n",
            "Epoch 37/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5071 - accuracy: 0.7584\n",
            "Epoch 38/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5101 - accuracy: 0.7402\n",
            "Epoch 39/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5118 - accuracy: 0.7486\n",
            "Epoch 40/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4972 - accuracy: 0.7612\n",
            "Epoch 41/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5044 - accuracy: 0.7528\n",
            "Epoch 42/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4794 - accuracy: 0.7626\n",
            "Epoch 43/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.5063 - accuracy: 0.7388\n",
            "Epoch 44/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4930 - accuracy: 0.7528\n",
            "Epoch 45/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4918 - accuracy: 0.7528\n",
            "Epoch 46/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4867 - accuracy: 0.7430\n",
            "Epoch 47/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4774 - accuracy: 0.7612\n",
            "Epoch 48/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4854 - accuracy: 0.7528\n",
            "Epoch 49/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4881 - accuracy: 0.7500\n",
            "Epoch 50/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4737 - accuracy: 0.7584\n",
            "Epoch 51/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4804 - accuracy: 0.7528\n",
            "Epoch 52/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4850 - accuracy: 0.7725\n",
            "Epoch 53/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4789 - accuracy: 0.7598\n",
            "Epoch 54/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4761 - accuracy: 0.7640\n",
            "Epoch 55/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4746 - accuracy: 0.7556\n",
            "Epoch 56/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4621 - accuracy: 0.7669\n",
            "Epoch 57/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4493 - accuracy: 0.7640\n",
            "Epoch 58/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4611 - accuracy: 0.7584\n",
            "Epoch 59/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4592 - accuracy: 0.7795\n",
            "Epoch 60/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4614 - accuracy: 0.7626\n",
            "Epoch 61/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4664 - accuracy: 0.7753\n",
            "Epoch 62/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4507 - accuracy: 0.7907\n",
            "Epoch 63/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4489 - accuracy: 0.7795\n",
            "Epoch 64/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4558 - accuracy: 0.7739\n",
            "Epoch 65/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4623 - accuracy: 0.7795\n",
            "Epoch 66/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4516 - accuracy: 0.7683\n",
            "Epoch 67/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4500 - accuracy: 0.7837\n",
            "Epoch 68/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4499 - accuracy: 0.7753\n",
            "Epoch 69/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4408 - accuracy: 0.7767\n",
            "Epoch 70/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4576 - accuracy: 0.7739\n",
            "Epoch 71/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4519 - accuracy: 0.7823\n",
            "Epoch 72/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4483 - accuracy: 0.7739\n",
            "Epoch 73/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4362 - accuracy: 0.7837\n",
            "Epoch 74/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4391 - accuracy: 0.7879\n",
            "Epoch 75/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4381 - accuracy: 0.7809\n",
            "Epoch 76/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4327 - accuracy: 0.7837\n",
            "Epoch 77/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4479 - accuracy: 0.7921\n",
            "Epoch 78/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4506 - accuracy: 0.7823\n",
            "Epoch 79/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4417 - accuracy: 0.7823\n",
            "Epoch 80/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4285 - accuracy: 0.7865\n",
            "Epoch 81/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4371 - accuracy: 0.7823\n",
            "Epoch 82/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4289 - accuracy: 0.7992\n",
            "Epoch 83/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4105 - accuracy: 0.8076\n",
            "Epoch 84/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4397 - accuracy: 0.7949\n",
            "Epoch 85/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4381 - accuracy: 0.7809\n",
            "Epoch 86/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4154 - accuracy: 0.8006\n",
            "Epoch 87/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4169 - accuracy: 0.7851\n",
            "Epoch 88/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4189 - accuracy: 0.7963\n",
            "Epoch 89/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4465 - accuracy: 0.7978\n",
            "Epoch 90/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4152 - accuracy: 0.7893\n",
            "Epoch 91/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.7893\n",
            "Epoch 92/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4460 - accuracy: 0.7739\n",
            "Epoch 93/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4216 - accuracy: 0.7907\n",
            "Epoch 94/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4275 - accuracy: 0.7963\n",
            "Epoch 95/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4229 - accuracy: 0.7865\n",
            "Epoch 96/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4196 - accuracy: 0.8048\n",
            "Epoch 97/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4279 - accuracy: 0.8048\n",
            "Epoch 98/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4216 - accuracy: 0.8090\n",
            "Epoch 99/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4224 - accuracy: 0.7949\n",
            "Epoch 100/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3972 - accuracy: 0.7963\n",
            "Epoch 101/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4163 - accuracy: 0.8034\n",
            "Epoch 102/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4237 - accuracy: 0.8146\n",
            "Epoch 103/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4152 - accuracy: 0.8104\n",
            "Epoch 104/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4031 - accuracy: 0.7921\n",
            "Epoch 105/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3960 - accuracy: 0.8020\n",
            "Epoch 106/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8230\n",
            "Epoch 107/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4096 - accuracy: 0.7837\n",
            "Epoch 108/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4126 - accuracy: 0.7963\n",
            "Epoch 109/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8076\n",
            "Epoch 110/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4091 - accuracy: 0.7992\n",
            "Epoch 111/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3974 - accuracy: 0.8132\n",
            "Epoch 112/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3853 - accuracy: 0.8188\n",
            "Epoch 113/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4218 - accuracy: 0.8076\n",
            "Epoch 114/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3945 - accuracy: 0.7978\n",
            "Epoch 115/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3957 - accuracy: 0.8258\n",
            "Epoch 116/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3815 - accuracy: 0.8230\n",
            "Epoch 117/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3870 - accuracy: 0.8174\n",
            "Epoch 118/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3936 - accuracy: 0.8062\n",
            "Epoch 119/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3989 - accuracy: 0.8118\n",
            "Epoch 120/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4125 - accuracy: 0.8062\n",
            "Epoch 121/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4034 - accuracy: 0.8188\n",
            "Epoch 122/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3910 - accuracy: 0.8006\n",
            "Epoch 123/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3797 - accuracy: 0.8343\n",
            "Epoch 124/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4193 - accuracy: 0.8048\n",
            "Epoch 125/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3716 - accuracy: 0.8090\n",
            "Epoch 126/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3818 - accuracy: 0.8202\n",
            "Epoch 127/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4011 - accuracy: 0.8076\n",
            "Epoch 128/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4083 - accuracy: 0.8062\n",
            "Epoch 129/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3769 - accuracy: 0.8090\n",
            "Epoch 130/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3775 - accuracy: 0.8146\n",
            "Epoch 131/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3798 - accuracy: 0.8216\n",
            "Epoch 132/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3834 - accuracy: 0.8160\n",
            "Epoch 133/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3957 - accuracy: 0.8090\n",
            "Epoch 134/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3755 - accuracy: 0.8076\n",
            "Epoch 135/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3657 - accuracy: 0.8230\n",
            "Epoch 136/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3859 - accuracy: 0.8188\n",
            "Epoch 137/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3863 - accuracy: 0.8160\n",
            "Epoch 138/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3736 - accuracy: 0.8230\n",
            "Epoch 139/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4022 - accuracy: 0.8160\n",
            "Epoch 140/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3721 - accuracy: 0.8216\n",
            "Epoch 141/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3693 - accuracy: 0.8118\n",
            "Epoch 142/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3911 - accuracy: 0.8174\n",
            "Epoch 143/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3727 - accuracy: 0.8174\n",
            "Epoch 144/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3830 - accuracy: 0.8118\n",
            "Epoch 145/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3711 - accuracy: 0.8146\n",
            "Epoch 146/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3732 - accuracy: 0.8118\n",
            "Epoch 147/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3693 - accuracy: 0.8216\n",
            "Epoch 148/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3768 - accuracy: 0.8090\n",
            "Epoch 149/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3759 - accuracy: 0.8230\n",
            "Epoch 150/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3748 - accuracy: 0.8244\n",
            "Epoch 151/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3862 - accuracy: 0.8202\n",
            "Epoch 152/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3604 - accuracy: 0.8343\n",
            "Epoch 153/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3994 - accuracy: 0.8146\n",
            "Epoch 154/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3785 - accuracy: 0.8343\n",
            "Epoch 155/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3811 - accuracy: 0.8132\n",
            "Epoch 156/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3707 - accuracy: 0.8202\n",
            "Epoch 157/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3632 - accuracy: 0.8188\n",
            "Epoch 158/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3672 - accuracy: 0.8287\n",
            "Epoch 159/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3766 - accuracy: 0.8202\n",
            "Epoch 160/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3821 - accuracy: 0.8076\n",
            "Epoch 161/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8132\n",
            "Epoch 162/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3765 - accuracy: 0.8090\n",
            "Epoch 163/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3888 - accuracy: 0.8090\n",
            "Epoch 164/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3650 - accuracy: 0.8188\n",
            "Epoch 165/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3878 - accuracy: 0.8315\n",
            "Epoch 166/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3669 - accuracy: 0.8329\n",
            "Epoch 167/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3863 - accuracy: 0.8216\n",
            "Epoch 168/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3610 - accuracy: 0.8090\n",
            "Epoch 169/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3733 - accuracy: 0.8329\n",
            "Epoch 170/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3749 - accuracy: 0.8104\n",
            "Epoch 171/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3824 - accuracy: 0.8076\n",
            "Epoch 172/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3584 - accuracy: 0.8244\n",
            "Epoch 173/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3499 - accuracy: 0.8343\n",
            "Epoch 174/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3631 - accuracy: 0.8230\n",
            "Epoch 175/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3572 - accuracy: 0.8301\n",
            "Epoch 176/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3774 - accuracy: 0.8146\n",
            "Epoch 177/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3464 - accuracy: 0.8244\n",
            "Epoch 178/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3710 - accuracy: 0.8202\n",
            "Epoch 179/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3608 - accuracy: 0.8174\n",
            "Epoch 180/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3634 - accuracy: 0.8272\n",
            "Epoch 181/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3602 - accuracy: 0.8160\n",
            "Epoch 182/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3521 - accuracy: 0.8343\n",
            "Epoch 183/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3644 - accuracy: 0.8230\n",
            "Epoch 184/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3571 - accuracy: 0.8329\n",
            "Epoch 185/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3769 - accuracy: 0.8315\n",
            "Epoch 186/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3466 - accuracy: 0.8301\n",
            "Epoch 187/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3457 - accuracy: 0.8357\n",
            "Epoch 188/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3406 - accuracy: 0.8427\n",
            "Epoch 189/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3598 - accuracy: 0.8272\n",
            "Epoch 190/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3685 - accuracy: 0.8272\n",
            "Epoch 191/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3693 - accuracy: 0.8301\n",
            "Epoch 192/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3484 - accuracy: 0.8343\n",
            "Epoch 193/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3490 - accuracy: 0.8329\n",
            "Epoch 194/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3487 - accuracy: 0.8301\n",
            "Epoch 195/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3522 - accuracy: 0.8301\n",
            "Epoch 196/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3867 - accuracy: 0.8216\n",
            "Epoch 197/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3368 - accuracy: 0.8329\n",
            "Epoch 198/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3739 - accuracy: 0.8315\n",
            "Epoch 199/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3964 - accuracy: 0.8244\n",
            "Epoch 200/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3279 - accuracy: 0.8511\n",
            "Epoch 201/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3586 - accuracy: 0.8104\n",
            "Epoch 202/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3766 - accuracy: 0.8329\n",
            "Epoch 203/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3402 - accuracy: 0.8315\n",
            "Epoch 204/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3453 - accuracy: 0.8287\n",
            "Epoch 205/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3504 - accuracy: 0.8329\n",
            "Epoch 206/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3523 - accuracy: 0.8497\n",
            "Epoch 207/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3361 - accuracy: 0.8371\n",
            "Epoch 208/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3277 - accuracy: 0.8539\n",
            "Epoch 209/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3475 - accuracy: 0.8272\n",
            "Epoch 210/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3883 - accuracy: 0.8258\n",
            "Epoch 211/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3375 - accuracy: 0.8343\n",
            "Epoch 212/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3370 - accuracy: 0.8399\n",
            "Epoch 213/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3392 - accuracy: 0.8343\n",
            "Epoch 214/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3429 - accuracy: 0.8216\n",
            "Epoch 215/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3463 - accuracy: 0.8301\n",
            "Epoch 216/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3418 - accuracy: 0.8455\n",
            "Epoch 217/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3256 - accuracy: 0.8385\n",
            "Epoch 218/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3330 - accuracy: 0.8469\n",
            "Epoch 219/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3421 - accuracy: 0.8315\n",
            "Epoch 220/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3514 - accuracy: 0.8399\n",
            "Epoch 221/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3488 - accuracy: 0.8385\n",
            "Epoch 222/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3660 - accuracy: 0.8132\n",
            "Epoch 223/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3529 - accuracy: 0.8399\n",
            "Epoch 224/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3341 - accuracy: 0.8399\n",
            "Epoch 225/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3312 - accuracy: 0.8329\n",
            "Epoch 226/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3389 - accuracy: 0.8525\n",
            "Epoch 227/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3733 - accuracy: 0.8287\n",
            "Epoch 228/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3716 - accuracy: 0.8329\n",
            "Epoch 229/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3449 - accuracy: 0.8343\n",
            "Epoch 230/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3459 - accuracy: 0.8385\n",
            "Epoch 231/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3430 - accuracy: 0.8483\n",
            "Epoch 232/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3328 - accuracy: 0.8553\n",
            "Epoch 233/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3101 - accuracy: 0.8511\n",
            "Epoch 234/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3486 - accuracy: 0.8357\n",
            "Epoch 235/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3164 - accuracy: 0.8413\n",
            "Epoch 236/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3582 - accuracy: 0.8343\n",
            "Epoch 237/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3509 - accuracy: 0.8399\n",
            "Epoch 238/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3466 - accuracy: 0.8371\n",
            "Epoch 239/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3298 - accuracy: 0.8301\n",
            "Epoch 240/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3304 - accuracy: 0.8469\n",
            "Epoch 241/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3503 - accuracy: 0.8413\n",
            "Epoch 242/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3208 - accuracy: 0.8385\n",
            "Epoch 243/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3267 - accuracy: 0.8567\n",
            "Epoch 244/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3498 - accuracy: 0.8511\n",
            "Epoch 245/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3322 - accuracy: 0.8385\n",
            "Epoch 246/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3517 - accuracy: 0.8469\n",
            "Epoch 247/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3481 - accuracy: 0.8399\n",
            "Epoch 248/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3216 - accuracy: 0.8399\n",
            "Epoch 249/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3365 - accuracy: 0.8315\n",
            "Epoch 250/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3137 - accuracy: 0.8525\n",
            "Epoch 251/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3568 - accuracy: 0.8399\n",
            "Epoch 252/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3526 - accuracy: 0.8329\n",
            "Epoch 253/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3360 - accuracy: 0.8455\n",
            "Epoch 254/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3113 - accuracy: 0.8497\n",
            "Epoch 255/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3248 - accuracy: 0.8399\n",
            "Epoch 256/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3245 - accuracy: 0.8357\n",
            "Epoch 257/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3091 - accuracy: 0.8399\n",
            "Epoch 258/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3419 - accuracy: 0.8483\n",
            "Epoch 259/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3235 - accuracy: 0.8455\n",
            "Epoch 260/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3312 - accuracy: 0.8525\n",
            "Epoch 261/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3253 - accuracy: 0.8427\n",
            "Epoch 262/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3341 - accuracy: 0.8343\n",
            "Epoch 263/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3137 - accuracy: 0.8385\n",
            "Epoch 264/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3163 - accuracy: 0.8413\n",
            "Epoch 265/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3253 - accuracy: 0.8497\n",
            "Epoch 266/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3109 - accuracy: 0.8469\n",
            "Epoch 267/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3182 - accuracy: 0.8483\n",
            "Epoch 268/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3404 - accuracy: 0.8301\n",
            "Epoch 269/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3213 - accuracy: 0.8539\n",
            "Epoch 270/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.2839 - accuracy: 0.8666\n",
            "Epoch 271/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3117 - accuracy: 0.8357\n",
            "Epoch 272/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3031 - accuracy: 0.8469\n",
            "Epoch 273/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3417 - accuracy: 0.8357\n",
            "Epoch 274/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3574 - accuracy: 0.8455\n",
            "Epoch 275/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3234 - accuracy: 0.8469\n",
            "Epoch 276/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3624 - accuracy: 0.8385\n",
            "Epoch 277/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3149 - accuracy: 0.8427\n",
            "Epoch 278/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.2964 - accuracy: 0.8708\n",
            "Epoch 279/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3422 - accuracy: 0.8315\n",
            "Epoch 280/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3272 - accuracy: 0.8497\n",
            "Epoch 281/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3191 - accuracy: 0.8469\n",
            "Epoch 282/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3169 - accuracy: 0.8581\n",
            "Epoch 283/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3110 - accuracy: 0.8525\n",
            "Epoch 284/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3342 - accuracy: 0.8511\n",
            "Epoch 285/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.2969 - accuracy: 0.8567\n",
            "Epoch 286/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3563 - accuracy: 0.8427\n",
            "Epoch 287/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3503 - accuracy: 0.8385\n",
            "Epoch 288/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3259 - accuracy: 0.8539\n",
            "Epoch 289/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.2959 - accuracy: 0.8638\n",
            "Epoch 290/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3084 - accuracy: 0.8455\n",
            "Epoch 291/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3047 - accuracy: 0.8399\n",
            "Epoch 292/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3142 - accuracy: 0.8581\n",
            "Epoch 293/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.2964 - accuracy: 0.8511\n",
            "Epoch 294/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3063 - accuracy: 0.8596\n",
            "Epoch 295/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3041 - accuracy: 0.8539\n",
            "Epoch 296/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3847 - accuracy: 0.8329\n",
            "Epoch 297/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.2840 - accuracy: 0.8511\n",
            "Epoch 298/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3310 - accuracy: 0.8624\n",
            "Epoch 299/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.3085 - accuracy: 0.8666\n",
            "Epoch 300/300\n",
            "712/712 [==============================] - 1s 1ms/step - loss: 0.2980 - accuracy: 0.8511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd8770de0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKtxZ2HYWZk0",
        "colab_type": "text"
      },
      "source": [
        "## Valutazione della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Jxqd-lUmaV",
        "colab_type": "code",
        "outputId": "d829a356-6aaf-4d20-bc93-ef607789e09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Valutazione del modello\n",
        "val_loss, val_acc = model.evaluate(val_dataset)\n",
        "print(\"\\nTest accuracy: {:.2f} ({:.2%})\".format(val_acc, val_acc))"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179/179 [==============================] - 0s 889us/step - loss: 0.8875 - accuracy: 0.7486\n",
            "\n",
            "Test accuracy: 0.75 (74.86%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwWviCzhZwfQ",
        "colab_type": "text"
      },
      "source": [
        "## Previsione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACgBSfpKZyRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBmIIEvzhfrY",
        "colab_type": "code",
        "outputId": "8e6278eb-0a02-4f08-88b0-20aa134523ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"passegerID \\t Predicted survival\\n\")\n",
        "for pid, prediction in zip(titanic_load_test['PassengerId'], predictions):\n",
        "  prediction = tf.sigmoid(prediction).numpy()\n",
        "  print(\"{} \\t\\t Predicted survival: {:.2%}\".format(pid, prediction[0]))"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passegerID \t Predicted survival\n",
            "\n",
            "0 \t\t Predicted survival: 0.00%\n",
            "1 \t\t Predicted survival: 0.00%\n",
            "2 \t\t Predicted survival: 0.06%\n",
            "3 \t\t Predicted survival: 0.00%\n",
            "4 \t\t Predicted survival: 99.36%\n",
            "5 \t\t Predicted survival: 11.03%\n",
            "6 \t\t Predicted survival: 0.00%\n",
            "7 \t\t Predicted survival: 0.22%\n",
            "8 \t\t Predicted survival: 27.26%\n",
            "9 \t\t Predicted survival: 0.03%\n",
            "10 \t\t Predicted survival: 2.73%\n",
            "11 \t\t Predicted survival: 0.01%\n",
            "12 \t\t Predicted survival: 100.00%\n",
            "13 \t\t Predicted survival: 100.00%\n",
            "14 \t\t Predicted survival: 0.00%\n",
            "15 \t\t Predicted survival: 38.02%\n",
            "16 \t\t Predicted survival: 0.39%\n",
            "17 \t\t Predicted survival: 0.00%\n",
            "18 \t\t Predicted survival: 0.30%\n",
            "19 \t\t Predicted survival: 0.00%\n",
            "20 \t\t Predicted survival: 1.41%\n",
            "21 \t\t Predicted survival: 4.00%\n",
            "22 \t\t Predicted survival: 99.99%\n",
            "23 \t\t Predicted survival: 0.64%\n",
            "24 \t\t Predicted survival: 94.29%\n",
            "25 \t\t Predicted survival: 0.00%\n",
            "26 \t\t Predicted survival: 100.00%\n",
            "27 \t\t Predicted survival: 0.00%\n",
            "28 \t\t Predicted survival: 0.00%\n",
            "29 \t\t Predicted survival: 0.65%\n",
            "30 \t\t Predicted survival: 0.01%\n",
            "31 \t\t Predicted survival: 0.12%\n",
            "32 \t\t Predicted survival: 96.67%\n",
            "33 \t\t Predicted survival: 99.99%\n",
            "34 \t\t Predicted survival: 7.50%\n",
            "35 \t\t Predicted survival: 0.01%\n",
            "36 \t\t Predicted survival: 71.79%\n",
            "37 \t\t Predicted survival: 98.45%\n",
            "38 \t\t Predicted survival: 0.70%\n",
            "39 \t\t Predicted survival: 100.00%\n",
            "40 \t\t Predicted survival: 80.85%\n",
            "41 \t\t Predicted survival: 97.59%\n",
            "42 \t\t Predicted survival: 0.00%\n",
            "43 \t\t Predicted survival: 99.87%\n",
            "44 \t\t Predicted survival: 3.14%\n",
            "45 \t\t Predicted survival: 0.06%\n",
            "46 \t\t Predicted survival: 0.45%\n",
            "47 \t\t Predicted survival: 18.21%\n",
            "48 \t\t Predicted survival: 99.53%\n",
            "49 \t\t Predicted survival: 99.98%\n",
            "50 \t\t Predicted survival: 0.00%\n",
            "51 \t\t Predicted survival: 13.84%\n",
            "52 \t\t Predicted survival: 98.18%\n",
            "53 \t\t Predicted survival: 99.79%\n",
            "54 \t\t Predicted survival: 0.46%\n",
            "55 \t\t Predicted survival: 0.06%\n",
            "56 \t\t Predicted survival: 0.00%\n",
            "57 \t\t Predicted survival: 0.00%\n",
            "58 \t\t Predicted survival: 0.15%\n",
            "59 \t\t Predicted survival: 92.34%\n",
            "60 \t\t Predicted survival: 2.76%\n",
            "61 \t\t Predicted survival: 0.00%\n",
            "62 \t\t Predicted survival: 0.83%\n",
            "63 \t\t Predicted survival: 82.33%\n",
            "64 \t\t Predicted survival: 0.01%\n",
            "65 \t\t Predicted survival: 38.50%\n",
            "66 \t\t Predicted survival: 78.82%\n",
            "67 \t\t Predicted survival: 0.00%\n",
            "68 \t\t Predicted survival: 0.02%\n",
            "69 \t\t Predicted survival: 96.07%\n",
            "70 \t\t Predicted survival: 45.75%\n",
            "71 \t\t Predicted survival: 0.14%\n",
            "72 \t\t Predicted survival: 86.69%\n",
            "73 \t\t Predicted survival: 0.00%\n",
            "74 \t\t Predicted survival: 99.91%\n",
            "75 \t\t Predicted survival: 0.00%\n",
            "76 \t\t Predicted survival: 34.92%\n",
            "77 \t\t Predicted survival: 0.00%\n",
            "78 \t\t Predicted survival: 0.00%\n",
            "79 \t\t Predicted survival: 90.86%\n",
            "80 \t\t Predicted survival: 30.28%\n",
            "81 \t\t Predicted survival: 0.51%\n",
            "82 \t\t Predicted survival: 0.09%\n",
            "83 \t\t Predicted survival: 42.44%\n",
            "84 \t\t Predicted survival: 24.94%\n",
            "85 \t\t Predicted survival: 24.34%\n",
            "86 \t\t Predicted survival: 54.95%\n",
            "87 \t\t Predicted survival: 99.24%\n",
            "88 \t\t Predicted survival: 92.47%\n",
            "89 \t\t Predicted survival: 25.24%\n",
            "90 \t\t Predicted survival: 39.13%\n",
            "91 \t\t Predicted survival: 6.03%\n",
            "92 \t\t Predicted survival: 94.92%\n",
            "93 \t\t Predicted survival: 40.16%\n",
            "94 \t\t Predicted survival: 0.02%\n",
            "95 \t\t Predicted survival: 0.20%\n",
            "96 \t\t Predicted survival: 0.00%\n",
            "97 \t\t Predicted survival: 0.22%\n",
            "98 \t\t Predicted survival: 68.14%\n",
            "99 \t\t Predicted survival: 0.02%\n",
            "100 \t\t Predicted survival: 6.88%\n",
            "101 \t\t Predicted survival: 0.00%\n",
            "102 \t\t Predicted survival: 26.33%\n",
            "103 \t\t Predicted survival: 0.03%\n",
            "104 \t\t Predicted survival: 28.84%\n",
            "105 \t\t Predicted survival: 14.22%\n",
            "106 \t\t Predicted survival: 0.20%\n",
            "107 \t\t Predicted survival: 18.20%\n",
            "108 \t\t Predicted survival: 7.57%\n",
            "109 \t\t Predicted survival: 0.00%\n",
            "110 \t\t Predicted survival: 0.00%\n",
            "111 \t\t Predicted survival: 65.06%\n",
            "112 \t\t Predicted survival: 100.00%\n",
            "113 \t\t Predicted survival: 70.49%\n",
            "114 \t\t Predicted survival: 99.19%\n",
            "115 \t\t Predicted survival: 2.34%\n",
            "116 \t\t Predicted survival: 1.05%\n",
            "117 \t\t Predicted survival: 71.61%\n",
            "118 \t\t Predicted survival: 0.09%\n",
            "119 \t\t Predicted survival: 99.72%\n",
            "120 \t\t Predicted survival: 97.54%\n",
            "121 \t\t Predicted survival: 13.05%\n",
            "122 \t\t Predicted survival: 88.36%\n",
            "123 \t\t Predicted survival: 0.32%\n",
            "124 \t\t Predicted survival: 7.51%\n",
            "125 \t\t Predicted survival: 69.78%\n",
            "126 \t\t Predicted survival: 0.41%\n",
            "127 \t\t Predicted survival: 20.46%\n",
            "128 \t\t Predicted survival: 0.00%\n",
            "129 \t\t Predicted survival: 0.17%\n",
            "130 \t\t Predicted survival: 0.70%\n",
            "131 \t\t Predicted survival: 0.00%\n",
            "132 \t\t Predicted survival: 58.64%\n",
            "133 \t\t Predicted survival: 0.25%\n",
            "134 \t\t Predicted survival: 0.00%\n",
            "135 \t\t Predicted survival: 0.31%\n",
            "136 \t\t Predicted survival: 0.01%\n",
            "137 \t\t Predicted survival: 0.62%\n",
            "138 \t\t Predicted survival: 99.84%\n",
            "139 \t\t Predicted survival: 0.00%\n",
            "140 \t\t Predicted survival: 8.72%\n",
            "141 \t\t Predicted survival: 51.38%\n",
            "142 \t\t Predicted survival: 0.41%\n",
            "143 \t\t Predicted survival: 0.00%\n",
            "144 \t\t Predicted survival: 2.56%\n",
            "145 \t\t Predicted survival: 0.06%\n",
            "146 \t\t Predicted survival: 97.02%\n",
            "147 \t\t Predicted survival: 3.38%\n",
            "148 \t\t Predicted survival: 3.38%\n",
            "149 \t\t Predicted survival: 0.00%\n",
            "150 \t\t Predicted survival: 86.46%\n",
            "151 \t\t Predicted survival: 2.53%\n",
            "152 \t\t Predicted survival: 0.00%\n",
            "153 \t\t Predicted survival: 13.71%\n",
            "154 \t\t Predicted survival: 14.87%\n",
            "155 \t\t Predicted survival: 27.31%\n",
            "156 \t\t Predicted survival: 98.61%\n",
            "157 \t\t Predicted survival: 58.91%\n",
            "158 \t\t Predicted survival: 0.00%\n",
            "159 \t\t Predicted survival: 98.87%\n",
            "160 \t\t Predicted survival: 65.07%\n",
            "161 \t\t Predicted survival: 2.54%\n",
            "162 \t\t Predicted survival: 98.55%\n",
            "163 \t\t Predicted survival: 0.31%\n",
            "164 \t\t Predicted survival: 0.05%\n",
            "165 \t\t Predicted survival: 6.13%\n",
            "166 \t\t Predicted survival: 0.68%\n",
            "167 \t\t Predicted survival: 41.56%\n",
            "168 \t\t Predicted survival: 98.99%\n",
            "169 \t\t Predicted survival: 92.72%\n",
            "170 \t\t Predicted survival: 4.79%\n",
            "171 \t\t Predicted survival: 0.04%\n",
            "172 \t\t Predicted survival: 0.31%\n",
            "173 \t\t Predicted survival: 0.31%\n",
            "174 \t\t Predicted survival: 0.00%\n",
            "175 \t\t Predicted survival: 82.21%\n",
            "176 \t\t Predicted survival: 87.23%\n",
            "177 \t\t Predicted survival: 0.04%\n",
            "178 \t\t Predicted survival: 62.13%\n",
            "179 \t\t Predicted survival: 56.86%\n",
            "180 \t\t Predicted survival: 5.23%\n",
            "181 \t\t Predicted survival: 0.11%\n",
            "182 \t\t Predicted survival: 99.93%\n",
            "183 \t\t Predicted survival: 10.07%\n",
            "184 \t\t Predicted survival: 96.84%\n",
            "185 \t\t Predicted survival: 1.21%\n",
            "186 \t\t Predicted survival: 82.97%\n",
            "187 \t\t Predicted survival: 0.97%\n",
            "188 \t\t Predicted survival: 2.05%\n",
            "189 \t\t Predicted survival: 0.05%\n",
            "190 \t\t Predicted survival: 0.57%\n",
            "191 \t\t Predicted survival: 99.10%\n",
            "192 \t\t Predicted survival: 0.41%\n",
            "193 \t\t Predicted survival: 0.00%\n",
            "194 \t\t Predicted survival: 28.47%\n",
            "195 \t\t Predicted survival: 0.02%\n",
            "196 \t\t Predicted survival: 30.33%\n",
            "197 \t\t Predicted survival: 56.49%\n",
            "198 \t\t Predicted survival: 2.21%\n",
            "199 \t\t Predicted survival: 65.88%\n",
            "200 \t\t Predicted survival: 60.50%\n",
            "201 \t\t Predicted survival: 0.94%\n",
            "202 \t\t Predicted survival: 0.22%\n",
            "203 \t\t Predicted survival: 89.46%\n",
            "204 \t\t Predicted survival: 2.21%\n",
            "205 \t\t Predicted survival: 40.59%\n",
            "206 \t\t Predicted survival: 68.39%\n",
            "207 \t\t Predicted survival: 1.43%\n",
            "208 \t\t Predicted survival: 99.36%\n",
            "209 \t\t Predicted survival: 0.34%\n",
            "210 \t\t Predicted survival: 0.41%\n",
            "211 \t\t Predicted survival: 5.80%\n",
            "212 \t\t Predicted survival: 41.05%\n",
            "213 \t\t Predicted survival: 80.99%\n",
            "214 \t\t Predicted survival: 12.46%\n",
            "215 \t\t Predicted survival: 4.75%\n",
            "216 \t\t Predicted survival: 53.13%\n",
            "217 \t\t Predicted survival: 0.00%\n",
            "218 \t\t Predicted survival: 57.47%\n",
            "219 \t\t Predicted survival: 0.86%\n",
            "220 \t\t Predicted survival: 99.70%\n",
            "221 \t\t Predicted survival: 0.61%\n",
            "222 \t\t Predicted survival: 99.85%\n",
            "223 \t\t Predicted survival: 0.69%\n",
            "224 \t\t Predicted survival: 98.95%\n",
            "225 \t\t Predicted survival: 69.49%\n",
            "226 \t\t Predicted survival: 0.67%\n",
            "227 \t\t Predicted survival: 65.90%\n",
            "228 \t\t Predicted survival: 0.16%\n",
            "229 \t\t Predicted survival: 9.62%\n",
            "230 \t\t Predicted survival: 56.26%\n",
            "231 \t\t Predicted survival: 98.10%\n",
            "232 \t\t Predicted survival: 0.17%\n",
            "233 \t\t Predicted survival: 1.10%\n",
            "234 \t\t Predicted survival: 0.21%\n",
            "235 \t\t Predicted survival: 0.63%\n",
            "236 \t\t Predicted survival: 0.00%\n",
            "237 \t\t Predicted survival: 0.96%\n",
            "238 \t\t Predicted survival: 65.18%\n",
            "239 \t\t Predicted survival: 77.96%\n",
            "240 \t\t Predicted survival: 100.00%\n",
            "241 \t\t Predicted survival: 99.27%\n",
            "242 \t\t Predicted survival: 7.67%\n",
            "243 \t\t Predicted survival: 0.87%\n",
            "244 \t\t Predicted survival: 22.01%\n",
            "245 \t\t Predicted survival: 0.58%\n",
            "246 \t\t Predicted survival: 91.82%\n",
            "247 \t\t Predicted survival: 0.44%\n",
            "248 \t\t Predicted survival: 74.93%\n",
            "249 \t\t Predicted survival: 72.91%\n",
            "250 \t\t Predicted survival: 70.39%\n",
            "251 \t\t Predicted survival: 19.78%\n",
            "252 \t\t Predicted survival: 22.28%\n",
            "253 \t\t Predicted survival: 0.74%\n",
            "254 \t\t Predicted survival: 0.51%\n",
            "255 \t\t Predicted survival: 6.28%\n",
            "256 \t\t Predicted survival: 12.38%\n",
            "257 \t\t Predicted survival: 0.49%\n",
            "258 \t\t Predicted survival: 71.06%\n",
            "259 \t\t Predicted survival: 0.82%\n",
            "260 \t\t Predicted survival: 1.10%\n",
            "261 \t\t Predicted survival: 0.71%\n",
            "262 \t\t Predicted survival: 74.26%\n",
            "263 \t\t Predicted survival: 34.87%\n",
            "264 \t\t Predicted survival: 8.88%\n",
            "265 \t\t Predicted survival: 1.08%\n",
            "266 \t\t Predicted survival: 0.00%\n",
            "267 \t\t Predicted survival: 5.74%\n",
            "268 \t\t Predicted survival: 65.72%\n",
            "269 \t\t Predicted survival: 1.26%\n",
            "270 \t\t Predicted survival: 9.80%\n",
            "271 \t\t Predicted survival: 5.10%\n",
            "272 \t\t Predicted survival: 94.26%\n",
            "273 \t\t Predicted survival: 57.27%\n",
            "274 \t\t Predicted survival: 0.01%\n",
            "275 \t\t Predicted survival: 91.05%\n",
            "276 \t\t Predicted survival: 10.83%\n",
            "277 \t\t Predicted survival: 16.42%\n",
            "278 \t\t Predicted survival: 1.31%\n",
            "279 \t\t Predicted survival: 24.72%\n",
            "280 \t\t Predicted survival: 22.22%\n",
            "281 \t\t Predicted survival: 8.45%\n",
            "282 \t\t Predicted survival: 64.66%\n",
            "283 \t\t Predicted survival: 75.52%\n",
            "284 \t\t Predicted survival: 47.33%\n",
            "285 \t\t Predicted survival: 9.29%\n",
            "286 \t\t Predicted survival: 8.18%\n",
            "287 \t\t Predicted survival: 67.60%\n",
            "288 \t\t Predicted survival: 0.00%\n",
            "289 \t\t Predicted survival: 4.67%\n",
            "290 \t\t Predicted survival: 35.76%\n",
            "291 \t\t Predicted survival: 69.09%\n",
            "292 \t\t Predicted survival: 0.01%\n",
            "293 \t\t Predicted survival: 3.52%\n",
            "294 \t\t Predicted survival: 0.29%\n",
            "295 \t\t Predicted survival: 0.99%\n",
            "296 \t\t Predicted survival: 90.90%\n",
            "297 \t\t Predicted survival: 35.48%\n",
            "298 \t\t Predicted survival: 80.54%\n",
            "299 \t\t Predicted survival: 0.83%\n",
            "300 \t\t Predicted survival: 0.70%\n",
            "301 \t\t Predicted survival: 40.07%\n",
            "302 \t\t Predicted survival: 0.53%\n",
            "303 \t\t Predicted survival: 1.32%\n",
            "304 \t\t Predicted survival: 65.90%\n",
            "305 \t\t Predicted survival: 74.60%\n",
            "306 \t\t Predicted survival: 24.27%\n",
            "307 \t\t Predicted survival: 1.30%\n",
            "308 \t\t Predicted survival: 77.29%\n",
            "309 \t\t Predicted survival: 53.51%\n",
            "310 \t\t Predicted survival: 1.74%\n",
            "311 \t\t Predicted survival: 1.33%\n",
            "312 \t\t Predicted survival: 4.66%\n",
            "313 \t\t Predicted survival: 38.28%\n",
            "314 \t\t Predicted survival: 96.16%\n",
            "315 \t\t Predicted survival: 64.45%\n",
            "316 \t\t Predicted survival: 2.41%\n",
            "317 \t\t Predicted survival: 6.07%\n",
            "318 \t\t Predicted survival: 1.04%\n",
            "319 \t\t Predicted survival: 4.52%\n",
            "320 \t\t Predicted survival: 1.19%\n",
            "321 \t\t Predicted survival: 0.00%\n",
            "322 \t\t Predicted survival: 1.12%\n",
            "323 \t\t Predicted survival: 78.55%\n",
            "324 \t\t Predicted survival: 99.94%\n",
            "325 \t\t Predicted survival: 32.96%\n",
            "326 \t\t Predicted survival: 95.47%\n",
            "327 \t\t Predicted survival: 20.69%\n",
            "328 \t\t Predicted survival: 25.06%\n",
            "329 \t\t Predicted survival: 10.80%\n",
            "330 \t\t Predicted survival: 82.37%\n",
            "331 \t\t Predicted survival: 12.99%\n",
            "332 \t\t Predicted survival: 0.01%\n",
            "333 \t\t Predicted survival: 79.54%\n",
            "334 \t\t Predicted survival: 1.44%\n",
            "335 \t\t Predicted survival: 29.58%\n",
            "336 \t\t Predicted survival: 2.05%\n",
            "337 \t\t Predicted survival: 0.03%\n",
            "338 \t\t Predicted survival: 8.65%\n",
            "339 \t\t Predicted survival: 0.02%\n",
            "340 \t\t Predicted survival: 5.75%\n",
            "341 \t\t Predicted survival: 2.42%\n",
            "342 \t\t Predicted survival: 9.75%\n",
            "343 \t\t Predicted survival: 96.32%\n",
            "344 \t\t Predicted survival: 3.30%\n",
            "345 \t\t Predicted survival: 32.75%\n",
            "346 \t\t Predicted survival: 7.74%\n",
            "347 \t\t Predicted survival: 0.52%\n",
            "348 \t\t Predicted survival: 8.36%\n",
            "349 \t\t Predicted survival: 90.93%\n",
            "350 \t\t Predicted survival: 92.88%\n",
            "351 \t\t Predicted survival: 9.98%\n",
            "352 \t\t Predicted survival: 19.14%\n",
            "353 \t\t Predicted survival: 97.83%\n",
            "354 \t\t Predicted survival: 49.72%\n",
            "355 \t\t Predicted survival: 0.00%\n",
            "356 \t\t Predicted survival: 72.11%\n",
            "357 \t\t Predicted survival: 0.04%\n",
            "358 \t\t Predicted survival: 5.43%\n",
            "359 \t\t Predicted survival: 39.04%\n",
            "360 \t\t Predicted survival: 2.05%\n",
            "361 \t\t Predicted survival: 95.92%\n",
            "362 \t\t Predicted survival: 90.20%\n",
            "363 \t\t Predicted survival: 1.97%\n",
            "364 \t\t Predicted survival: 94.11%\n",
            "365 \t\t Predicted survival: 40.73%\n",
            "366 \t\t Predicted survival: 40.61%\n",
            "367 \t\t Predicted survival: 93.75%\n",
            "368 \t\t Predicted survival: 98.68%\n",
            "369 \t\t Predicted survival: 39.50%\n",
            "370 \t\t Predicted survival: 6.21%\n",
            "371 \t\t Predicted survival: 98.00%\n",
            "372 \t\t Predicted survival: 0.00%\n",
            "373 \t\t Predicted survival: 2.62%\n",
            "374 \t\t Predicted survival: 90.92%\n",
            "375 \t\t Predicted survival: 98.90%\n",
            "376 \t\t Predicted survival: 25.02%\n",
            "377 \t\t Predicted survival: 7.11%\n",
            "378 \t\t Predicted survival: 5.49%\n",
            "379 \t\t Predicted survival: 11.26%\n",
            "380 \t\t Predicted survival: 4.76%\n",
            "381 \t\t Predicted survival: 3.02%\n",
            "382 \t\t Predicted survival: 60.63%\n",
            "383 \t\t Predicted survival: 43.20%\n",
            "384 \t\t Predicted survival: 15.17%\n",
            "385 \t\t Predicted survival: 97.89%\n",
            "386 \t\t Predicted survival: 2.21%\n",
            "387 \t\t Predicted survival: 0.32%\n",
            "388 \t\t Predicted survival: 5.32%\n",
            "389 \t\t Predicted survival: 8.25%\n",
            "390 \t\t Predicted survival: 84.38%\n",
            "391 \t\t Predicted survival: 77.42%\n",
            "392 \t\t Predicted survival: 4.46%\n",
            "393 \t\t Predicted survival: 0.80%\n",
            "394 \t\t Predicted survival: 3.55%\n",
            "395 \t\t Predicted survival: 97.52%\n",
            "396 \t\t Predicted survival: 4.06%\n",
            "397 \t\t Predicted survival: 97.15%\n",
            "398 \t\t Predicted survival: 2.65%\n",
            "399 \t\t Predicted survival: 0.00%\n",
            "400 \t\t Predicted survival: 94.74%\n",
            "401 \t\t Predicted survival: 5.94%\n",
            "402 \t\t Predicted survival: 99.39%\n",
            "403 \t\t Predicted survival: 42.54%\n",
            "404 \t\t Predicted survival: 26.83%\n",
            "405 \t\t Predicted survival: 98.96%\n",
            "406 \t\t Predicted survival: 5.63%\n",
            "407 \t\t Predicted survival: 21.43%\n",
            "408 \t\t Predicted survival: 65.90%\n",
            "409 \t\t Predicted survival: 56.88%\n",
            "410 \t\t Predicted survival: 65.90%\n",
            "411 \t\t Predicted survival: 96.60%\n",
            "412 \t\t Predicted survival: 15.25%\n",
            "413 \t\t Predicted survival: 3.10%\n",
            "414 \t\t Predicted survival: 98.02%\n",
            "415 \t\t Predicted survival: 32.28%\n",
            "416 \t\t Predicted survival: 7.79%\n",
            "417 \t\t Predicted survival: 41.04%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}